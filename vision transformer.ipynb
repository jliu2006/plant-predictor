{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7db7e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# official code (JAX) from several Google transformer papers: https://github.com/google-research/vision_transformer\n",
    "# code taken from mildlyoverfitted's tutorial: https://www.youtube.com/watch?v=ovB0ddFtzzA&ab_channel=mildlyoverfitted\n",
    "# PyTorch code used in tutorial with pretrained weights: https://github.com/huggingface/pytorch-image-models\n",
    "\n",
    "import torch # pytorch 2.0.1 (https://pytorch.org/get-started/pytorch-2.0/)\n",
    "import torch.nn as nn    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36785ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    # splits each image into linear projections, or patches, so that the transformer can learn from them\n",
    "    # PARAMS:\n",
    "        # img_size: size of input image\n",
    "        # patch_size: size of each patch\n",
    "        # channels: num of input channels\n",
    "        # embed_dim: embedding dimension\n",
    "    \n",
    "    # ATTRIBUTES:\n",
    "        # patches: num of patches per image\n",
    "        # proj (nn.Conv2d): convolutional layer that splits image into patches and embeds\n",
    "    \n",
    "    def __init__(self, img_size, patch_size, channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            channels, \n",
    "            embed_dim, \n",
    "            kernel_size = patch_size, \n",
    "            stride = patch_size) # both kernel size and stride are equal to the patch size \n",
    "                                 # so that there will never be overlapping patches\n",
    "    def forward(self, x):\n",
    "        # run a forward pass. a tensor simply represents a batch of images.\n",
    "        # PARAMS:\n",
    "            # x (torch.Tensor): shape (n_samples, channels, img_size, img_size) for a square\n",
    "        \n",
    "        # OUTPUT: a 3D tensor representing the set of resulting patches\n",
    "            # torch.Tensor: shape (n_samples, n_patches, embed_dim)\n",
    "        \n",
    "        x = self.proj(\n",
    "                x # by running the input tensor through the Conv2d layer, we will get a 4D tensor\n",
    "        ) # shape (n_samples, embed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
    "        x = x.flatten(2) # flattens the tensor from the 2nd axis onward\n",
    "                         # eg., (n_patches ** 0.5) * (n_patches ** 0.5) = n_patches\n",
    "        x = x.transpose(1, 2) # swap 1st and 2nd axes\n",
    "    \n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    # mechanism for generating attention matrix\n",
    "    # PARAMS:\n",
    "        # dim: the input and output dimensions of per token features\n",
    "        # n_heads: number of attention heads\n",
    "        # qkv_bias: bool, if a bias value is included in the query, key, and value (qkv) projections\n",
    "        # attn_p: Dropout probability (ratio) applied to the qkv tensors\n",
    "        # proj_p: Dropout probability applied to the output tensor\n",
    "    \n",
    "    # ATTRIBUTES:\n",
    "        # scale: normalizing constant for dot product (attention matrix)\n",
    "        # qkv (nn.Linear): linear projection for qkv\n",
    "        # proj (nn.Linear): linear mapping that takes concatenated attention matrix as input and maps into a new space\n",
    "        # attn_drop, proj_drop: Dropout layers for qkv and output\n",
    "    \n",
    "    def __init__(self, token_dim, n_heads=12, qkv_bias=True, attn_p=0, proj_p=0):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "        # head dimensions are specified in this way so that once the resulting attention heads are concantenated,\n",
    "        # the new tensor will have the same dimensions as the input\n",
    "        \n",
    "        self.scale = self.head_dim ** -0.5 \n",
    "        # this scaling value comes from the \"Attention is All you Need\" paper\n",
    "        # its purpose is to prevent very large values from being fed into the softmax layer,\n",
    "        # which would otherwise cause small gradients\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias = qkv_bias)\n",
    "        # linear mapping that will take token embedding as an input and produce qkv values\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        # linear mapping that takes concatenated heads as an input and maps into new space\n",
    "        \n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # run forward pass. the +1 to n_patches is to ensure that the class token of each sample\n",
    "        # is always included as the first token in the sequence\n",
    "        # PARAMS:\n",
    "            # x (torch.Tensor): shape (n_samples, patches + 1, dim)\n",
    "        \n",
    "        # OUTPUT (same shape):\n",
    "            # torch.Tensor: shape (n_samples, patches + 1, dim)\n",
    "        \n",
    "        n_samples, n_tokens, dim = x.shape\n",
    "        \n",
    "        if dim != self.dim: # check whether input embedding dimension matches declared dimension in constructor\n",
    "            raise ValueError\n",
    "        \n",
    "        qkv = self.qkv(x) # (samples, patches + 1, 3 * dim)\n",
    "        # take input tensor and convert to qkv    \n",
    "        qkv = qkv.reshape(n_samples, n_tokens, 3, self.n_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        # change order of dimensions to (3, n_samples, n_heads, n_patches + 1, head_dim)\n",
    "        \n",
    "        q, k, v = qkv[0], qkv[1], qkv[2] # extraction\n",
    "        k_t = k.transpose(-2, -1) # transpose keys in preparation for dot product\n",
    "        dp = (q @ k_t) * self.scale\n",
    "        # compute dot product attention matrix with scale factor\n",
    "        # shape (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
    "        attn = dp.softmax(dim=-1) # create discrete probability distribution that adds to 1\n",
    "        # can be used as weights, forms a weighted average\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        weighted_avg = attn @ v # compute weighted avg of all values\n",
    "        weighted_avg = weighted_avg.transpose(1, 2)\n",
    "        # shape (n_samples, n_patches + 1, n_heads, head_dim)\n",
    "        \n",
    "        weighted_avg = weighted_avg.flatten(2) # flatten last 2 dimensions\n",
    "        # has the net effect of concatenating all attention heads\n",
    "        # left with a 3D tensor: (n_samples, n_patches + 1, dim)\n",
    "        \n",
    "        x = self.proj(weighted_avg)\n",
    "        x = self.proj_drop(x)\n",
    "        # apply linear layer\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
