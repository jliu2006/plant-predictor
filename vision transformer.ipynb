{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7db7e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# official code (JAX) from several Google transformer papers: https://github.com/google-research/vision_transformer\n",
    "# code taken from mildlyoverfitted's tutorial: https://www.youtube.com/watch?v=ovB0ddFtzzA&ab_channel=mildlyoverfitted\n",
    "# PyTorch code used in tutorial with pretrained weights: https://github.com/huggingface/pytorch-image-models\n",
    "\n",
    "import torch # pytorch 2.0.1 (https://pytorch.org/get-started/pytorch-2.0/)\n",
    "import torch.nn as nn    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b88ff39f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3852456927.py, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [13], line 27\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    # splits each image into linear projections, or patches, so that the transformer can learn from them\n",
    "    # PARAMS:\n",
    "        # img_size: size of input image\n",
    "        # patch_size: size of each patch\n",
    "        # channels: num of input channels\n",
    "        # dims: embedding dimension\n",
    "    \n",
    "    # ATTRIBUTES:\n",
    "        # patches: num of patches per image\n",
    "        # proj (nn.Conv2d): convolutional layer that splits image into patches and embeds\n",
    "    \n",
    "    def __init__(self, img_size, patch_size, channels=3, dims=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            channels, \n",
    "            dims, \n",
    "            kernel_size = patch_size, \n",
    "            stride = patch_size) # both kernel size and stride are equal to the patch size \n",
    "                                 # so that there will never be overlapping patches\n",
    "    def forward(self, x):\n",
    "        # run a forward pass. a tensor simply represents a batch of images.\n",
    "        # PARAMS:\n",
    "            # x (torch.Tensor): shape (n_samples, channels, img_size, img_size) for a square\n",
    "        \n",
    "        # OUTPUT: a 3D tensor representing the set of resulting patches\n",
    "            # torch.Tensor: shape (n_samples, n_patches, dims)\n",
    "        \n",
    "        x = self.proj(\n",
    "                x # by running the input tensor through the Conv2d layer, we will get a 4D tensor\n",
    "        ) # shape (n_samples, dims, n_patches ** 0.5, n_patches ** 0.5)\n",
    "        x = x.flatten(2) # flattens the tensor from the 2nd axis onward\n",
    "                         # eg., (n_patches ** 0.5) * (n_patches ** 0.5) = n_patches\n",
    "        x = x.transpose(1, 2) # swap 1st and 2nd axes\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
