{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85f337d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'type' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 80\u001b[0m\n\u001b[1;32m     77\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m SpatialImageDataset(Dataset, targets)  \u001b[38;5;66;03m# Replace with your training dataset class\u001b[39;00m\n\u001b[1;32m     78\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m SpatialImageDataset(Dataset, targets)    \u001b[38;5;66;03m# Replace with your test dataset class\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Instantiate your spatiotemporal transformer model\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:351\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 351\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/sampler.py:106\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement))\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_samples\u001b[49m, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue, but got num_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/sampler.py:114\u001b[0m, in \u001b[0;36mRandomSampler.num_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnum_samples\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# dataset size might change at runtime\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples\n",
      "Cell \u001b[0;32mIn [7], line 16\u001b[0m, in \u001b[0;36mSpatialImageDataset.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'type' has no len()"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Define your spatiotemporal transformer model\n",
    "class SpatiotemporalTransformer(nn.Module):\n",
    "    def __init__(self, num_frames=1, num_channels=1, hidden_dim=1, # ask about this, maybe not necessary\n",
    "                 num_heads=1, num_layers=1):\n",
    "#         super(SpatiotemporalTransformer, self).__init__()\n",
    "\n",
    "#         # Spatial Transformer\n",
    "#         self.spatial_transformer = nn.TransformerEncoderLayer(d_model=num_channels, nhead=num_heads)\n",
    "\n",
    "#         # Temporal Transformer\n",
    "#         self.temporal_transformer = nn.TransformerEncoderLayer(d_model=num_channels, nhead=num_heads)\n",
    "\n",
    "#         # Positional encodings for spatial and temporal dimensions\n",
    "#         self.spatial_pos_enc = nn.Parameter(torch.zeros(num_frames, num_channels))\n",
    "#         self.temporal_pos_enc = nn.Parameter(torch.zeros(num_frames, num_channels))\n",
    "\n",
    "#         # Fully connected layer for pixel prediction\n",
    "#         self.fc = nn.Linear(num_channels, 3)  # Assuming RGB images\n",
    "\n",
    "    def forward(self, x): # callable in nn.Module source code, runs whenever class is called with input\n",
    "        \n",
    "#         batch_size, num_frames, num_channels, height, width = x.size()\n",
    "\n",
    "#         # Reshape the input to be compatible with the transformer\n",
    "#         x = x.view(batch_size * num_frames, num_channels, height, width)\n",
    "\n",
    "#         # Apply spatial transformation\n",
    "#         x = x.permute(2, 3, 0, 1)  # (height, width, batch_size*num_frames, num_channels)\n",
    "#         spatial_pos_enc = self.spatial_pos_enc.unsqueeze(1).repeat(1, height * width, 1)  # (num_frames, height*width, num_channels)\n",
    "#         x = x + spatial_pos_enc\n",
    "#         x = x.view(height * width, batch_size * num_frames, num_channels)  # (height*width, batch_size*num_frames, num_channels)\n",
    "#         x = self.spatial_transformer(x)\n",
    "#         x = x.view(height, width, batch_size, num_frames, num_channels)\n",
    "#         x = x.permute(2, 3, 4, 0, 1)  # (batch_size, num_frames, num_channels, height, width)\n",
    "\n",
    "#         # Apply temporal transformation\n",
    "#         x = x.view(batch_size, num_frames, -1)  # (batch_size, num_frames, num_channels*height*width)\n",
    "#         temporal_pos_enc = self.temporal_pos_enc.unsqueeze(1).repeat(1, num_frames, 1)  # (num_frames, num_frames, num_channels)\n",
    "#         x = x + temporal_pos_enc\n",
    "#         x = x.permute(1, 0, 2)  # (num_frames, batch_size, num_channels*height*width)\n",
    "#         x = self.temporal_transformer(x)\n",
    "#         x = x.permute(1, 0, 2)  # (batch_size, num_frames, num_channels*height*width)\n",
    "\n",
    "#         # Pixel prediction\n",
    "#         x = x.view(batch_size * num_frames, -1)  # (batch_size*num_frames, num_channels*height*width)\n",
    "#         x = self.fc(x)\n",
    "#         x = x.view(batch_size, num_frames, 3, height, width)  # (batch_size, num_frames, 3, height, width)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Vegetation(Dataset): # inherited\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file) # replace with our file-reading\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Set up your dataset and data loaders\n",
    "train_dataset = Vegetation(Dataset, targets)  # Replace with your training dataset class\n",
    "test_dataset = Vegetation(Dataset, targets)    # Replace with your test dataset class\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "# Instantiate your spatiotemporal transformer model\n",
    "model = SpatiotemporalTransformer()\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Set the device (CPU or GPU) for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # forward()?\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Print training loss for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss}\")\n",
    "\n",
    "    # Model evaluation\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    # Print test loss for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Test Loss: {test_loss}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"spatiotemporal_transformer.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
